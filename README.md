# ğŸ›°ï¸ Titan: A Distributed Job Orchestrator

**Titan** is a **hybrid distributed orchestrator** designed for small-to-medium scale environments.

It synthesizes the core capabilities of batch schedulers (DAGs, dependencies) and infrastructure managers (dynamic scaling) into a single, unified runtime.

### Philosophy
Titan is not designed to replace enterprise-grade tools like Kubernetes or Airflow. Instead, it offers a **lightweight alternative** for scenarios where running a full cluster stack is overkill. It strictly focuses on the *essential* primitives of orchestrationâ€”resolving dependencies and managing worker lifecyclesâ€”without the operational complexity, heavy dependencies, or steep learning curve of the larger ecosystems.

### Core Capabilities
Titan provides a streamlined environment for diverse workloads:
* **Universal Execution:** Runs one-off operational scripts, long-running services, and compiled binaries in the same cluster.
* **Hybrid DAGs:** Orchestrates dependency chains where a single DAG can mix ephemeral scripts, persistent services, and infrastructure provisioning tasks.
* **Autonomous Operations:** Features built-in **self-scaling** (workers spawning new workers to handle burst loads) and **self-healing** (automatic job retries).

*Built as a systems engineering initiative, Titan explores the "hard parts" of distributed computingâ€”custom RPC protocols, consensus, and state managementâ€”implemented from scratch in Java with **zero external dependencies**.*

## [INFO] Key Features

- **Dual-Layer Orchestration**:
    - **Workload Layer**: Manages complex job dependencies, retries, and scheduling priorities.
    - **Infrastructure Layer**: Workers act as infrastructure agents, capable of spawning new nodes (containers/processes) on demand to handle burst loads.
- **DAG-Based Scheduling**: Supports complex dependency chains (e.g., *Task A â†’ Task B & C â†’ Task D*). It handles "Waiting Rooms" for blocked jobs and executes them automatically when dependencies resolve.
- **Recursive Scaling**: Workers can spawn new child processes (other Workers) on demand. This allows the cluster to scale horizontally on a single machine to utilize available CPU cores during high load.
- **Hybrid Payloads**: Native support for diverse payloads without external runners:
    -  **Scripts**: Python/Shell (Base64 encoded transfer).
    -  **Binaries**: JAR/Binary propagation (Bytecode distribution).
    -  **Ops Tasks**: Email alerts, PDF conversions, DB cleanup signals.
- **Custom TCP Protocol**: Implements a custom wire protocol (`TITAN_PROTOCOL`) using fixed-header framing (Version + OpCode + Length). Designed to handle TCP fragmentation and avoid the overhead of text-based protocols like JSON.
- **Real-time Dashboard**: A lightweight Flask (Python) UI to visualize node health, active job queues, and real-time logs..

## ğŸ› ï¸ Architecture

The system follows a **Leader-Follower** topology with a decoupled control plane:

1. **Scheduler (Master - Port 9090):**
    - Role: The "Brain" of the cluster.
    - Responsibilities:
        - Parses Job DAGs and builds execution trees.
        - Manages the JobQueue and WaitingRoom (for tasks pending dependencies).
        - Assigns tasks using a Least-Connections load-balancing algorithm.
        - Maintains cluster state consistency via WAL.
2. **RpcWorker (Node):**
    - The "Muscle" of the cluster. Executes generic payloads.
    - Responsibilities:
      - Executes payloads (Python scripts, Shell commands, JARs).
      - Streams generic logs via UDP to the aggregation layer.
      - Listens for "Inception" commands to spawn child nodes.
3. **The Protocol:**
   - Communication between Master and Workers happens over a raw TCP socket using TITAN_PROTO:
   [ HEADER (8 Bytes) ]
   | Version (1B) | OpCode (1B) | Flags (1B) | Spare (1B) | Payload Length (4B) |
   [ BODY ]
   | Binary Payload (Variable) ... |

4. **Zero-Dependency Architecture:**
   - No external database: Manages state in-memory with a custom Write-Ahead Log (WAL) for crash recovery and durability.
   - No external networking libs: Built on raw Java java.nio and java.net sockets.


##  File System

Titan uses a strictly defined directory structure to manage payload distribution and execution results:

- **`perm_files/`**: The "Source of Truth." Place all your scripts (`.py`, `.sh`) and binaries (`.jar`) here.
    - *Note:* While the system supports in-flight Base64 payload injection, Titan defaults to reading from `perm_files` to ensure reliability and reproducibility.
- **`titan_workspace/`**: The "Output Sandbox." All execution results, logs, and artifacts generated by the workers are saved here.

##  Getting Started

### Prerequisites
- Java JDK 17+
- Python 3.x (for Dashboard)

### Installation
```bash
git clone [https://github.com/yourusername/titan-orchestrator.git](https://github.com/yourusername/titan-orchestrator.git)
javac -d bin src/**/*.java
```


## Quick Start (Local)
### Running the Cluster

1. **Start Master:** `java -cp bin network.SchedulerServer`
2. **Start Worker:** `java -cp bin network.RpcWorkerServer 8080`
3. **Start CLI:** `java -cp bin client.TitanCLI`


### Easy way alternative


If you prefer running without Docker, open **three separate terminals**:

1. **Terminal 1 (Scheduler):**
```bash
java scheduler.TitanMaster
# Output: [OK] SchedulerServer Listening on port 9090

```


2. **Terminal 2 (Worker):**
```bash
java network.TitanWorker
# Output: [OK] Successfully registered with Scheduler!

```


3. **Terminal 3 (CLI):**
```bash
java client.TitanCli
# Output: Connected to localhost:9090

```


## Quick Start (Docker)

The easiest way to run a full cluster (1 Scheduler + 3 Workers) is using Docker Compose.

**1. Create docker-compose.yml**

```yaml
version: "3.8"

services:
  titan-scheduler:
    build: .
    command:
      - java
      - -cp
      - out/production/DistributedOrchestrator
      - scheduler.TitanMaster
    ports:
      - "9090:9090"

  worker-1:
    build: .
    command:
      - java
      - -cp
      - out/production/DistributedOrchestrator
      - network.TitanWorker
    depends_on:
      - titan-scheduler
```

**2. Run the Cluster**

```bash
docker-compose up --build
```

**3. Connect via CLI**

```bash
java -cp out/production/DistributedOrchestrator client.TitanCli
```


---

##  Usage & Commands

### 1. The "Inception" DAG (Recursive Scaling)

This command demonstrates the system's power. It deploys a dashboard, spawns two new workers in parallel, and runs a calculation script once the workers are ready.

```bash
SUBMIT_DAG START_DASH|DEPLOY|server_dashboard.py|1|0|[] ; WORKER_A|DEPLOY|Worker.jar|8081|1|2|[START_DASH] ; WORKER_B|DEPLOY|Worker.jar|8082|1|2|[START_DASH] ; RUN_CALC|RUN|calc.py|2|0|[WORKER_A, WORKER_B] ; WORKER_C|DEPLOY|Worker.jar|8083|1|5|[RUN_CALC]

```

### 2. Advanced One-Off Jobs

You can submit standalone jobs with delays (scheduling) and priorities directly from the CLI.

**Format:** `SUBMIT TYPE|DATA|PRIORITY|DELAY_MS`

```bash
# Standard Immediate Execution
titan> SUBMIT RUN|calc.py

# Scheduled Execution (Run 8 seconds from now)
# Useful for delayed reports or cleanup tasks
titan> SUBMIT PDF_CONVERT|future_report.pdf|1|8000

# Hypothetical Task (If handler is implemented)
titan> SUBMIT EMAIL_ALERT|admin@titan.com|1|0

```

### 3. System Stats

View real-time cluster health. Below is an example output showing a **Recursive Scale-Up** event where 2 initial workers spawned 3 additional nodes.


```text
titan> STATS

--- TITAN SYSTEM MONITOR ---
Active Workers:    5
Execution Queue:   1 jobs
Delayed (Time):    0 jobs
Blocked (DAG):     0 jobs
Dead Letter (DLQ): 0 jobs
-------------------------------
Worker Status:
 â€¢ [8081] Load: 0/4 (0%)     | Skills: [GENERAL]
    â””â”€â”€ âš™ï¸ Service ID: WORKER_B
    â””â”€â”€ âš™ï¸ Service ID: WORKER_C
 â€¢ [8082] Load: 0/4 (0%)     | Skills: [GENERAL]
 â€¢ [8080] Load: 0/4 (0%)     | Skills: [GENERAL]
    â””â”€â”€ âš™ï¸ Service ID: WORKER_A
    â””â”€â”€ âš™ï¸ Service ID: TSK-435fb042...
    â””â”€â”€ âš™ï¸ Service ID: START_DASH
 â€¢ [8088] Load: 0/4 (0%)     | Skills: [GENERAL]
 â€¢ [8083] Load: 0/4 (0%)     | Skills: [GENERAL]

```

---

##  Dashboard

Titan includes a Python Flask dashboard to visualize the cluster.

```bash
python3 dashboard.py
# Running on http://localhost:5000

```

##  Roadmap

* [ ] **Persistence (WAL):** Implement Write-Ahead Logging to restore cluster state after a Master crash.
* [ ] **Leader Election:** Implement the Bully Algorithm to handle Master node failures automatically.
* [ ] **Security:** Add TLS/SSL encryption to the `TitanProtocol`.

---

**Author:** Ram Narayanan A S
*Built to understand the "Hard Parts" of Distributed Systems.*
