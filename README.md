# ğŸ›°ï¸ Titan: A Distributed Job Orchestrator

**Titan** is a **hybrid infrastructure** and **workload orchestrator** built from scratch in Java.

Unlike traditional job schedulers that merely execute tasks on static hardware, Titan functions as a **self-provisioning platform**. It blends the workload management capabilities of tools like Apache Airflow (DAGs, dependencies) with the node orchestration capabilities of Kubernetes (dynamic scaling, self-replication).

It doesn't just run your code; it intelligently expands the cluster infrastructure to meet the demands of your code.

## ğŸš€ Key Features

- **Dual-Layer Orchestration**:
    - **Workload Layer**: Manages complex job dependencies, retries, and scheduling priorities.
    - **Infrastructure Layer**: Workers act as infrastructure agents, capable of spawning new nodes (containers/processes) on demand to handle burst loads.
- **DAG Execution Engine**: Supports complex dependency chains (e.g., *Task A â†’ Task B & C â†’ Task D*). It handles "Waiting Rooms" for blocked jobs and executes them automatically when dependencies resolve.
- **Recursive Scaling ("Inception")**: A unique capability where Workers have the intelligence to spawn *new* Worker nodes dynamically based on workload instructions.
- **Hybrid Payloads**: A unified pipeline that handles diverse task types:
    - ğŸ **Script Execution**: Python/Shell scripts (Source distribution + Base64 encoding).
    - ğŸ“¦ **Deployment**: JAR/Binary propagation (Bytecode distribution).
    - âš¡ **Logical Tasks**: Email alerts, PDF conversions, DB cleanup signals.
- **Custom TCP Protocol**: Efficient, binary-packed communication layer (`TITAN_PROTOCOL`) with "Smart Parsing" for variable-length payloads.
- **Real-time Dashboard**: A Flask-based (Python) UI for visualizing cluster health, load, and active services.

## ğŸ› ï¸ Architecture

The system follows a classic **Master-Slave (Leader-Follower)** architecture:

1. **Scheduler (Master - Port 9090):**
    - The "Brain" of the cluster. Parses DAGs, manages the `JobQueue`, and assigns tasks via a **Least-Connections Load Balancing** strategy.

2. **RpcWorker (Node):**
    - The "Muscle" of the cluster. Executes generic payloads.
    - **Self-Replication**: Can execute a `DEPLOY` job to spin up a child process (another Worker) on a different port, effectively scaling the cluster horizontally on demand.



## ğŸ“‚ File System Strategy

Titan uses a strictly defined directory structure to manage payload distribution and execution results:

- **`perm_files/`**: The "Source of Truth." Place all your scripts (`.py`, `.sh`) and binaries (`.jar`) here.
    - *Note:* While the system supports in-flight Base64 payload injection, Titan defaults to reading from `perm_files` to ensure reliability and reproducibility.
- **`titan_workspace/`**: The "Output Sandbox." All execution results, logs, and artifacts generated by the workers are saved here.

## ğŸ’» Getting Started

### Prerequisites
- Java JDK 17+
- Python 3.x (for Dashboard)

### Installation
```bash
git clone [https://github.com/yourusername/titan-orchestrator.git](https://github.com/yourusername/titan-orchestrator.git)
javac -d bin src/**/*.java
```


## Quick Start (Local)
### Running the Cluster

1. **Start Master:** `java -cp bin network.SchedulerServer`
2. **Start Worker:** `java -cp bin network.RpcWorkerServer 8080`
3. **Start CLI:** `java -cp bin client.TitanCLI`


### Easy way alternative


If you prefer running without Docker, open **three separate terminals**:

1. **Terminal 1 (Scheduler):**
```bash
java scheduler.TitanMaster
# Output: âœ… SchedulerServer Listening on port 9090

```


2. **Terminal 2 (Worker):**
```bash
java network.TitanWorker
# Output: âœ… Successfully registered with Scheduler!

```


3. **Terminal 3 (CLI):**
```bash
java client.TitanCli
# Output: Connected to localhost:9090

```


## Quick Start (Docker)

The easiest way to run a full cluster (1 Scheduler + 3 Workers) is using Docker Compose.

**1. Create docker-compose.yml**

```yaml
version: "3.8"

services:
  titan-scheduler:
    build: .
    command:
      - java
      - -cp
      - out/production/DistributedOrchestrator
      - scheduler.TitanMaster
    ports:
      - "9090:9090"

  worker-1:
    build: .
    command:
      - java
      - -cp
      - out/production/DistributedOrchestrator
      - network.TitanWorker
    depends_on:
      - titan-scheduler
```

**2. Run the Cluster**

```bash
docker-compose up --build
```

**3. Connect via CLI**

```bash
java -cp out/production/DistributedOrchestrator client.TitanCli
```


---

## ğŸ® Usage & Commands

### 1. The "Inception" DAG (Recursive Scaling)

This command demonstrates the system's power. It deploys a dashboard, spawns two new workers in parallel, and runs a calculation script once the workers are ready.

```bash
SUBMIT_DAG START_DASH|DEPLOY|server_dashboard.py|1|0|[] ; WORKER_A|DEPLOY|Worker.jar|8081|1|2|[START_DASH] ; WORKER_B|DEPLOY|Worker.jar|8082|1|2|[START_DASH] ; RUN_CALC|RUN|calc.py|2|0|[WORKER_A, WORKER_B] ; WORKER_C|DEPLOY|Worker.jar|8083|1|5|[RUN_CALC]

```

### 2. Advanced One-Off Jobs

You can submit standalone jobs with delays (scheduling) and priorities directly from the CLI.

**Format:** `SUBMIT TYPE|DATA|PRIORITY|DELAY_MS`

```bash
# Standard Immediate Execution
titan> SUBMIT RUN|calc.py

# Scheduled Execution (Run 8 seconds from now)
# Useful for delayed reports or cleanup tasks
titan> SUBMIT PDF_CONVERT|future_report.pdf|1|8000

# Hypothetical Task (If handler is implemented)
titan> SUBMIT EMAIL_ALERT|admin@titan.com|1|0

```

### 3. System Stats

View real-time cluster health. Below is an example output showing a **Recursive Scale-Up** event where 2 initial workers spawned 3 additional nodes.


```text
titan> STATS

--- ğŸ›°ï¸ TITAN SYSTEM MONITOR ---
Active Workers:    5
Execution Queue:   1 jobs
Delayed (Time):    0 jobs
Blocked (DAG):     0 jobs
Dead Letter (DLQ): 0 jobs
-------------------------------
Worker Status:
 â€¢ [8081] Load: 0/4 (0%)     | Skills: [GENERAL]
    â””â”€â”€ âš™ï¸ Service ID: WORKER_B
    â””â”€â”€ âš™ï¸ Service ID: WORKER_C
 â€¢ [8082] Load: 0/4 (0%)     | Skills: [GENERAL]
 â€¢ [8080] Load: 0/4 (0%)     | Skills: [GENERAL]
    â””â”€â”€ âš™ï¸ Service ID: WORKER_A
    â””â”€â”€ âš™ï¸ Service ID: TSK-435fb042...
    â””â”€â”€ âš™ï¸ Service ID: START_DASH
 â€¢ [8088] Load: 0/4 (0%)     | Skills: [GENERAL]
 â€¢ [8083] Load: 0/4 (0%)     | Skills: [GENERAL]

```

---

## ğŸ“Š Dashboard

Titan includes a Python Flask dashboard to visualize the cluster.

```bash
python3 dashboard.py
# Running on http://localhost:5000

```

## ğŸš§ Roadmap

* [ ] **Persistence (WAL):** Implement Write-Ahead Logging to restore cluster state after a Master crash.
* [ ] **Leader Election:** Implement the Bully Algorithm to handle Master node failures automatically.
* [ ] **Security:** Add TLS/SSL encryption to the `TitanProtocol`.

---

**Author:** Ram Narayanan A S
*Built to understand the "Hard Parts" of Distributed Systems.*
